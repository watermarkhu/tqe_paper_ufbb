\section{Introduction}\label{sec:introduction}
One of the most promising approaches for fault-tolerant quantum computation is based on surface quantum error-correcting codes \cite{dennis2002topological, kitaev2003fault}. With surface codes, error correction only requires the measurement of local operators on a two-dimensional lattice of qubits. The set of measurement outcomes, called the syndrome, is passed to the decoding algorithm to infer the error that has occurred and to supply a correction operator. The resilience against errors can be improved by increasing the system size if the physical error rate is below a threshold value $p_{\text{th}}$. For this increased resiliency to hold, it is essential that the decoder has low time complexity; if the clock-rate of the quantum computer becomes limited by the decoder, the advantages of increasing the system size could be compromised.

Many decoding algorithms have been developed that either aim to improve the threshold and lower logical error rates \cite{wang2003confinement, raussendorf2007faulttolerant, fowler2012towards, fowler2013minimum, heim2016optimal, duclos2010fast, duclos2013fault, bravyi2014efficient, darmawan2018linear}, or to perform under more realistic noise models \cite{tuckett2020fault, hutter2015improved, bravyi2013quantum,  nickerson2019analysing, wootton2012high, huang2020fault}, including a new class of neural network decoders \cite{baireuther2019neural, chamberland2018deep, liu2019neural, nautrup2019optimizing, torlai2017neural, varsamopoulos2017decoding, varsamopoulos2020decoding}. %, and other types \cite{bombin2012universal, herold2015cellular, horsman2012surface, kubica2019cellular, watson2015fast}. 
The most popular decoder for surface codes is the Minimum-Weight Perfect Matching (MWPM) decoder. It performs near-optimal for a bit-flip noise model \cite{dennis2002topological} on a standard toric code with a threshold of $p_{\text{th}} = 10.3\%$, and for a phenomenological noise model \cite{wang2003confinement}, which includes faulty measurements, with $p_{\text{th}} = 2.9\%$. The main idea is to approximate the error with the minimum-weight error configuration compatible with the syndrome. The minimum-weight configuration is found by constructing a fully connected graph between the nodes of the syndrome, which leads to a cubic worst-case time complexity \cite{kolmogorov2009blossom}. Recent empirical results show that, in some regimes, MWPM can be performed with worst case linear complexity \cite{fowler2012towards} but further work is necessary to achieve the throughput required for real-time error correction.

In this work, we build on top of a recently proposed decoder called the Union-Find (UF) decoder. It combines a low worst-case time complexity with a high threshold \cite{delfosse2017almost}, making it a practical solution for real devices \cite{huang2020fault,delfosse2020hierarchical,das2020scalable}. The UF decoder's thresholds on the toric code with independent bit-flip noise and phenomenological noise are $9.9\%$ and $2.7\%$, and its worst-case time complexity is $\mathcal{O}(n\alpha(n))$, where $\alpha$ is the inverse of Ackermann's function \cite{tarjan1975efficiency}. For any physically feasible amount of qubits, this value is $\alpha(n) \leq 3$, leading to an ``almost-linear'' time complexity. 

Our goal is to bridge the gap between MWPM and UF while maintaining a quasi-linear complexity. For this, we first investigate several small modifications of the UF decoder and notice that the best performance is achieved by the modifications that output matchings with minimal weight. Leveraging this idea, we propose a modified UF decoder that heuristically attempts to minimize the weight of the matchings. The modified decoder, which we dub the \emph{Union-Find Partitioned-Growth decoder} (UFPG), performs similar to the MWPM decoder while maintaining a quasi-linear time complexity. In \Cref{sec:surfacecode,sec:unionfind}, we introduce the surface code and the UF decoder. In \Cref{sec:ufbb}, we describe the modified algorithm and its motivation. We discuss the algorithm's complexity in \Cref{sec:complexity} and compare its performance with other decoders in \Cref{sec:performance}.  

\section{The Surface Code}\label{sec:surfacecode}

The \emph{toric code}, a topological code introduced by Kitaev \cite{kitaev2003fault}, is defined as an arrangement of qubits on the edges of a square lattice with periodic boundary conditions. We denote by $V,E,F$ the set of vertices, set of edges and set of faces of the lattice. The code is defined as the ground state of the Hamiltonian
\begin{equation}
    H = -\sum_{v \in V} X_v -\sum_{f \in F} Z_f, 
\end{equation}
where the operator $X_v$ is the product of Pauli $X$ operators on the qubits located on the edges incident to the vertex $v$, \emph{i.e.}, $X_v = \prod_{e \in v} X_e$, and the operator $Z_f = \prod_{e \in f} Z_e$ is the product of Pauli $Z$ operators on the qubits located on edges of face $f$. The code space is spanned by the simultaneous ``+1'' eigenstate of all operators $X_v$ and $Z_f$. Together with any possible product of them, these operators are the \emph{stabilizers} of the code and form the stabilizer group $S$. The torus' non-trivial cycles encode the logical operators. Below a certain threshold, physical errors mostly introduce local effects that do not add up to a non-trivial cycle. 

For simplicity, we consider noise caused by independent and identically distributed (i.i.d.) bit-flip errors, where each qubit is subjected to a Pauli $X$ error with probability $p_X$. Due to {lattice duality}, the error detection and correction of phase-flip errors are identical. We consider a phenomenological model where the ideal measurements outputs are flipped also with probability $p_X$. 
%Additionally, any qubit may be \emph{erased} from the system with probability $p_e$. The set of erased qubits is denoted with $\varepsilon$. This \emph{erasure} is detectable, such that we can replace or reinitiate all erased qubits, which corresponds to a random Pauli error after measurement. 

Error correction is preceded by measuring a set of stabilizers of the code, \emph{i.e.}, the operators $X_v$ and $Z_f$. For a set of bit-flip errors $E_X = \{I,X\}^{\otimes n}$, the stabilizers $Z_f$ that anticommute with the error return a non-trivial outcome. The set of non-trivial eigenvalues of the stabilizers is called the syndrome $\sigma$ of the code. Given the measured $\sigma$, it is the task of the decoder to find the correction operator $\mathcal{C}(\sigma)$. When the correction operator is applied, the code is returned to the code space. %Since the error is corrected up to a stabilizer, the mapping of the syndrome to the correction is not one-to-one. The task of the decoder is to choose the correction most similar to the error. 

\section{Union-Find decoder}\label{sec:unionfind}
The Union-Find decoder \cite{delfosse2017almost} maps each element of the syndrome $\sigma$ to a vertex $v$ in a non-connected graph defined on the code lattice. From this starting point, it grows clusters around these vertices by repeatedly adding a layer of edges and vertices to existing clusters, until all clusters have an even number of non-trivial syndrome vertices. %This process is described as the growth of a cluster or the growth of the vertices that lie on a cluster's boundary. 
Then, it selects a spanning tree $F$ for each cluster. %, such that each cluster becomes a connected acyclic graph. 
The leaves of each spanning tree are conditionally peeled in a tail-recursive breadth-first search until all non-trivial syndrome vertices are paired and linked by a path within $F$, which is the correcting operator $\mathcal{C}$ \cite{delfosse2017almost}. The strategy for constructing the clusters turns out to have a strong effect on performance. For instance, the threshold for bit-flip noise of a decoder that grows the clusters following a random order is $9.2\%$ \cite{delfosse2017almost}, while if the clusters are grown in order of cluster size, which we call \textbf{Weighted Growth}, the threshold increases to $9.9\%$ \cite{delfosse2017almost}. %for bit-flip noise in this \textbf{Weighted Growth} variant of the decoder.

The complexity of the Union-Find decoder is driven by the merging of the clusters. For this, the algorithm uses the Union-Find or disjoint-set data structure \cite{tarjan1975efficiency}. This data structure contains a set of elements, in this case vertices on the lattice. The set of elements is represented by a two-level tree. At the root of the tree sits one element chosen arbitrarily; the rest of the elements are linked to the root element. The structure admits two functions: find and union. Given $v$ an element from the structure, the function $\Find(v)$ returns the root element of the tree. This is is used to %travel in the cluster-tree --- a disjoint set of the cluster's vertices --- from vertex $v$ to the representative element, to 
identify the cluster to which $v$ belongs. The second function is $\Union(u, v)$, this function merges the sets associated with elements $u$ and $v$. This requires pointing all the elements of one of the sets to the root of the other. In order to minimize the number of operations the root of the set with the larger number of elements is chosen as root for the merged set, this is called \textbf{Weighted Union}. In this context, $\Union$ is used when the growth of a cluster requires adding a vertex that belongs to another. % cluster, two vertices $u, v$ are connected on a newly added edge $(u,v)$, $\Find(u), \Find(v)$ output the roots $r_u, r_v$. If $r_u \neq r_v$, the two cluster-trees are not connected. The two cluster are then merged by $\Union(r_u, r_v)$ by pointing one tree's root to another root.  