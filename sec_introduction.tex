\section{Introduction}\label{sec:introduction}
One of the most promising approaches for fault-tolerant quantum computation is based on surface quantum error-correcting codes \cite{dennis2002topological, kitaev2003fault}. With surface codes, error correction only requires the measurement of local operators on a two-dimensional lattice of qubits. The set of measurement outcomes, called the syndrome, is passed to the decoding algorithm to infer the error that has occurred and to supply a correction operator. The resilience against errors can be improved by increasing the system size if the physical error rate is below a threshold value $p_{\text{th}}$. For this, it is essential that the decoder has low time complexity; if the clock-rate of the quantum computer becomes limited by the decoder, the advantages of increasing the system size could be compromised.

Many decoding algorithms have been developed that either aim to improve the threshold and lower logical error rates \cite{wang2003confinement, raussendorf2007faulttolerant, fowler2012towards, fowler2013minimum, heim2016optimal, duclos2010fast, duclos2013fault, bravyi2014efficient, darmawan2018linear}, or to perform under more realistic noise models \cite{tuckett2020fault, hutter2015improved, bravyi2013quantum,  nickerson2019analysing, wootton2012high, huang2020fault}, including a new class of neural network decoders \cite{baireuther2019neural, chamberland2018deep, liu2019neural, nautrup2019optimizing, torlai2017neural, varsamopoulos2017decoding, varsamopoulos2020decoding}. %, and other types \cite{bombin2012universal, herold2015cellular, horsman2012surface, kubica2019cellular, watson2015fast}. 
The most popular decoder for surface codes is the Minimum-Weight Perfect Matching (MWPM) decoder. It performs near-optimal for a bit-flip noise model \cite{dennis2002topological} on a standard toric code with a threshold of $p_{\text{th}} = 10.3\%$, and for a phenomenological noise model \cite{wang2003confinement}, which includes faulty measurements, with $p_{\text{th}} = 2.9\%$. The main idea is to approximate the error with the minimum-weight error configuration compatible with the syndrome. The minimum-weight configuration is found by constructing a fully connected graph between the nodes of the syndrome, which leads to a cubic worst-case time complexity of $\mathcal{O}(n^3)$, where $n$ is the number of qubits in the system \cite{kolmogorov2009blossom}. Recent empirical results show that, in some regimes, MWPM can be performed with worst case linear complexity \cite{fowler2012towards} but further work is necessary to achieve the processing time required for real-time error correction.

In this work, we build on top of a recently proposed decoder called the Union-Find (UF) decoder. It combines a low worst-case time complexity with a high threshold \cite{delfosse2017almost}, making it a practical solution for real devices \cite{huang2020fault,delfosse2020hierarchical,das2020scalable}. The UF decoder's thresholds on the toric code with independent bit-flip noise and phenomenological noise are $9.9\%$ and $2.7\%$, and its worst-case time complexity is $\mathcal{O}(n\alpha(n))$, where $\alpha$ is the inverse of Ackermann's function \cite{tarjan1975efficiency}. For any physically feasible amount of qubits, this value is $\alpha(n) \leq 3$, leading to an ``almost-linear'' time complexity. 

Our goal is to bridge the gap between MWPM and UF while maintaining a quasi-linear complexity. For this, we first investigate several small modifications of the UF decoder and notice that the best performance is achieved by the modifications that output matchings with minimal weight. Leveraging this idea, we propose a modified UF decoder that heuristically attempts to minimize the weight of the matchings. The modified decoder, which we dub the \emph{Union-Find Node-Suspension decoder} (UFNS), performs similar to the MWPM decoder while maintaining a quasi-linear time complexity. In \Cref{sec:surfacecode,sec:unionfind}, we introduce the surface code and the UF decoder. In \Cref{sec:ufbb}, we describe the modified algorithm and its motivation. We discuss the algorithm's complexity in \Cref{sec:complexity} and compare its performance with other decoders in \Cref{sec:performance}.  

\section{The Surface Code}\label{sec:surfacecode}

The \emph{toric code}, a topological code introduced by Kitaev \cite{kitaev2003fault}, is defined as an arrangement of qubits on the edges of a square latttice with periodic boundary conditions. The code is denoted by the set of vertices $V$, set of edges $E$ and set of faces $F$ of the lattice. The code is defined as the ground state of the Hamiltonian
\begin{equation}
    H = -\sum_{v \in V} X_v -\sum_{f \in F} Z_f, 
\end{equation}
where operator $X_v$ is the product of Pauli $X$ operators on the qubits located on edges forming the vertex $v$, \emph{i.e.}, $X_v = \prod_{e \in v} X_e$, and $Z_f = \prod_{e \in f} Z_e$ is the product of Pauli $Z$ operators on the qubits located on edges of face $f$. The code space is spanned by the simultaneous ``+1'' eigenstate of all operators $X_v$ and $Z_f$. Together with any possible product of them, these operators are the \emph{stabilizers} of the code and form the stabilizer group $S$. The torus' non-trivial cycles encode the logical operators. Below a certain threshold, physical errors will only introduce local effects and do not change these cycles that encode the logical information. 

For simplicity, we consider independent or non-correlated noise caused by i.i.d. bit-flip errors, where each qubit is subjected to a Pauli $X$ error with probability $p_X$. Due to \emph{lattice duality}, the error detection and correction of phase-flip errors are identical. The phenomenological noise model adds noisy measurements with the probability of error during each measurement equal to $p_X$. 
%Additionally, any qubit may be \emph{erased} from the system with probability $p_e$. The set of erased qubits is denoted with $\varepsilon$. This \emph{erasure} is detectable, such that we can replace or reinitiate all erased qubits, which corresponds to a random Pauli error after measurement. 

Error correction is proceeded by measuring a set of independent stabilizers of the code, \emph{i.e.}, the operators $X_v$ and $Z_f$. For a set of phase-flip errors $E_Z = \{I,Z\}^{\otimes n}$, the stabilizers $X_v$ that anticommute with the error return a non-trivial outcome. The set of non-trivial eigenvalues of the stabilizers is called the syndrome $\sigma$ of the code. Given the measured $\sigma$, it is the task of the decoder to find the correction operator $\mathcal{C}(\sigma)$. When the correction operator is applied, the code is returned to the code space, \emph{i.e.}, $\mathcal{C}(\sigma)E_Z \in S$. Since the error is corrected up to a stabilizer, the mapping of the syndrome to the correction is thus not one-to-one. It is up to the decoder to choose the correction most similar to the error. 

\section{Union-Find decoder}\label{sec:unionfind}

The Union-Find decoder \cite{delfosse2017almost} maps each element of the syndrome $\sigma$ to a so-called non-trivial vertex $v$ in a non-connected graph on the code lattice, and grows clusters that form a connected graph $G(V_i, E_i)$ of vertices $V_i\in V$ and edges $E_i \in E$ locally, by repeatedly adding a layer of edges and trivial vertices to existing clusters, until all clusters have an even number of non-trivial syndrome vertices. This process is described as the growth of a cluster or the growth of the vertices that lie on a cluster's boundary. Then, a spanning tree $F$ is built, such that each cluster is a connected acyclic graph., Leaves of the trees are conditionally peeled in a tail-recursive breadth-first search until all non-trivial syndrome vertices are paired and linked by a path within $F$, which is the correcting operator $\mathcal{C}$ \cite{delfosse2017linear}. By growing the clusters of vertices in order of their sizes --- the number of vertices in the cluster --- the threshold is reported to increase from $9.2\%$ to $9.9\%$ for bit-flip noise in this \textbf{Weighted Growth} variant of the decoder.

The merging between clusters drives the complexity of the Union-Find decoder. For this, the algorithm uses the Union-Find or disjoint-set data structure \cite{tarjan1975efficiency}. The function $\Find(v)$ is used to travel in the cluster-tree --- a disjoint set of the cluster's vertices --- from vertex $v$ to the representative root element $r_v$, to identify the cluster to which $v$ belongs. When two vertices $u, v$ are connected on a newly added edge $(u,v)$, $\Find(u), \Find(v)$ output the roots $r_u, r_v$. If $r_u \neq r_v$, the two cluster-trees are not connected. The two cluster are then mergeds by $\Union(r_u, r_v)$ by pointing one tree's root to another root. \textbf{Weighted Union} is performed by pointing the smaller tree to the larger to reduce the cost of future calls to $\Find$.