\section{Union-Find Partitioned-Growth decoder}\label{sec:ufbb}
In this section, we describe the \emph{Union-Find Partitioned-Growth} decoder. We first introduce the concept of the potential matching weight in \Cref{sec:matchingweight}. The Union-Find Partitioned-Growth decoder uses the potential matching weight as a heuristic to prioritize cluster growth. We introduce the node-tree data structure required for this decoder in \Cref{sec:nodeset}, and describe how to approximate the potential matching weight leveraging the node-tree in \Cref{sec:paritydelay}. In \Cref{sec:grownodetrees,sec:nodejoin,sec:inversion}, we find ways to further reduce the number of calculations needed in the approximation. The algorithm is described in \Cref{sec:pseudocode}. 

\subsection{Potential Matching Weight}\label{sec:matchingweight}

$\nup, \ndown, \mup, \mdown, n,m \nset$

\Figure[htb](topskip=0pt, botskip=0pt, midskip=0pt){figures/tikz/build/main-figure0.pdf}{
    A cluster with vertices $\{a,b,c\}$ with potential matching weights $\{2\frac{1}{2}, 3\frac{1}{2}, 2\frac{1}{2}\}$. The line style and color of the colored edges correspond to the matching in the hypothetical union with an external vertex $v'$ of the same line style and color.\label{fig0}}

%In the following we give some intuition into the improvement of the Union-Find Balanced Bloom decoder upon the original Union-Find decoder. 
% We compared the ratio of the matchings between the MWPM decoder and our own implementation of the UF decoder, averaged over many simulations, and found that UF matching weight has a constant prefactor of $\sim 1.043$ over the minimum weight for the toric code (\Cref{comp_weight}). From this, we suspected that a decreased matching weight is a heuristic for an increased threshold. Within the context of the UF decoder, the matching weight may be decreased by prioritizing the growth of vertices with low PWM's within the cluster. 

Consider a cluster containing the set of an odd number of non-trivial vertices $V=\{a,b,c\}$ and the set of edges $E=\{(a,b), (b, c)\}$ of \Cref{fig0}. Now let us investigate the weight of a matching if an additional non-trivial vertex $w$ is connected to the cluster. If $w$ is connected to $a$ or to $c$, then the resulting matching has a total weight of 2: $(w,a)$ and $(b,c)$, or $(a,b)$ and $(c,w)$. However, if $w$ is connected to vertex $b$, then the total weight is 3: $(w, b)$ and $(a, c)$. %If we want to minimize the weight of the 

Inspired by this observation, we associate with each vertex $v$ of an odd-parity cluster a \textbf{Potential Matching Weight} $\text{PMW}(v)$. The PMW measures the matching weight, assuming a union occurs in the next growth iteration. More precisely,
%\begin{definition}
    % define cluster
    % define matching size or weight
let $v$ be a vertex in the boundary of an odd cluster and let $w$ be a hypothetical non-trivial vertex adjacent to $v$ but exterior to the cluster. We define $\text{PMW}(v)$ as the minimum-weight perfect-matching's weight of the even cluster that results from adding the $(v,w)$ vertex to the odd cluster. % is given by the 
    %Let there be a hypothetical merger between odd cluster $\alpha$ of vertices $V_\alpha$ and edges $E_\alpha$, and odd cluster $\beta$ of $V_\beta$ and $E_\beta$, on the edge $(v_\alpha, v_\beta)$, where $v_\alpha \in V_\alpha$ and $v_\beta \in V_\beta$. In the merged even cluster with edges $E_{\gamma} = E_\alpha \cup E_\beta \cup (v_\alpha, v_\beta)$, there is a matching $\m{C}_{(v_\alpha,v_\beta)} \subseteq E_{\gamma}$  between the syndrome vertices internal to the cluster. The \textbf{Potential Matching Weight} (PMW) of vertex $v_\alpha$ is then defined as
    %\begin{equation}
      %\text{PMW}(v) = \abs{\m{C}_{(v_\alpha,v_\beta)} \cap E_\alpha} + 1.
     % \text{PMW}(v) = \abs{\m{C'}}.
    %\end{equation}
    %\end{definition}
    
Since per growth iteration half-edges are attached to the odd cluster's boundary, the addition of an edge $(v,w)$ to a cluster occurs in two steps. In the first step, half of $(v,w)$ is added to the cluster of $v$. If $w$ is part of another cluster, and if the other half of $(v,w)$ was already added to the cluster of $w$, $(v,w)$ is now \emph{fully grown}. If not, another round of growth is required to merge the clusters of $v$ and $w$. The PWM of $v$ is identical in both cases. To be able to distinguish between to two cases, we add $\nicefrac{1}{2}$ to the PMW's of vertices with half-edges attached. Using this definition, the cluster with vertices $\{a,b,c\}$ in \Cref{fig0} has PMW's $\{2\nicefrac{1}{2}, 3\nicefrac{1}{2}, 2\nicefrac{1}{2}\}$. The PMW can be utilized to prioritize the growth of vertices with low PMW such that there is an increased probability of mergers between clusters on edges connected to these vertices, and there is an increased probability of a lower matching weight. However, this heuristic is only interesting if the PMW' calculation is lighter computationally than performing minimum-weight perfect-matching. %within a cluster is potentially , especially for clusters of increasingly larger size, as all edges of a cluster must be considered in its calculation. Furthermore, the PMWs within a cluster change due to cluster growth and mergers, both of which occur more frequently as the system size increases. For this reason, the scaling of the PMW computation is vital to the decoder. 


\Figure[htb](topskip=0pt, botskip=0pt, midskip=0pt){figures/tikz/build/main-figure1.pdf}{
    The cluster of \Cref{fig0}, comprised of nodes $\{A, B, C\}$ with respective primer vertices $\{a, b, c\}$, after two rounds of prioritized growth of $a$ and $c$. There are regions of vertices that are either interior elements or have equal potential matching weights, represented as nodes with different node radii (labelled below the node) in the node-tree $\nset$. \label{fig1}}

\subsection{Partitioned-Growth data structure}\label{sec:nodeset}
Fortunately, the PMW can be approximated efficiently. For this we introduce the \textbf{Partitioned-Growth} data structure for a cluster $C$.

Let the sub-graph $\vset\subseteq C$ be a spanning-tree of $C$ if it is a connected acyclic sub-graph that includes all vertices of $C$ and a minimum number of edges. A node-tree $\nset$ is a partition of the spanning-tree $\vset$, such that each element of the partition --- which we call a \textbf{node} --- represents a set of adjacent vertices that lie within a certain distance --- the \textbf{node radius} $\rho$ --- from the \textbf{primer vertex}, which initializes the node and lies at its center. The node-tree is an undirected acyclic graph, and its edges $\m{E}_i$ have lengths equal to the distance (edges in $\vset$) between the primer vertices of neighboring nodes. 


%\begin{definition}

%\end{definition}

%Consider the cluster of non-trivial vertices $V_i=\{v_0,v_1,v_2\}$ and edges $E_i = \{(v_0,v_1), (v_1, v_2)\}$ from \Cref{fig0}. We had found previously that vertices $v_0, v_2$ have a lower PMW compared to $v_1$ by 1 edge. The growth of $v_0$ and $v_2$ could be prioritized, such that new vertices are added to the cluster on the boundary of $v_0$ and $v_2$. If all newly added vertices are trivial, the cluster would be described by \Cref{fig1}. If we repeat the PMW calculation, we now find that the PMWs of the new vertices connected to $v_0$ are equal. The same holds for the vertices connected to $v_2$.

Suppose every non-trivial vertex is the primer vertex of a node, all vertices within a node that lie at distance $r$ to the primer vertex are either boundary vertices to the cluster and have equal PMW, or lie within the radius of another node. Additionally, a growth iteration can be represented as an increase in the node radius by $\nicefrac{1}{2}$. The weight of a matching in $\vset$ is thus equal to the weight of the same matching in $\nset$. For the example in \Cref{fig1}, the PMW of all boundary vertices of $A$, henceforth just the PMW of $A$, is $\rho_A + \abs{(B,C)} +1$. The partitioning of $\vset$ to $\nset$ thus allows us to compute the PMW using parameters of the reduced node-tree. We dub all nodes that have non-trivial vertices as primer vertices \textbf{syndrome-nodes} $\dot{N}$. Additionally, trivial vertices that lie on equal radii of two or more different nodes initiate a different type \textbf{junction-nodes} $\bar{N}$. A cluster with a node-tree comprised of both node types is depicted in \Cref{fig2}. Note that similarly to trivial vertices on the cluster, junction-nodes do not count towards the parity of the node-tree. 

\Figure[htb](topskip=0pt, botskip=0pt, midskip=0pt){figures/tikz/build/main-figure2.pdf}{
Two different node types. Syndrome-nodes $N$ have a non-trivial vertex or syndrome at its center. Vertices that lie on the radii of two existing nodes initialize a junction-node $\bar{N}$ in the node-tree. Vertex $b$ lies on equal distances to primer vertices $a$ of node $\dot{A}$ and $c$ of $\dot{C}$, and vertex $d$ to primer vertices $c$ of $\dot{C}$ and $e$ of $\dot{E}$. Therefore, vertices $b$ and $d$ initiate junction-nodes $\bar{B}$ and $\bar{D}$, respectively, in the merged node-tree. \label{fig2}}

\Figure[hbt!](topskip=0pt, botskip=0pt, midskip=0pt){figures/tikz/build/main-figure3.pdf}{
    The relevant data structures. \emph{(a)} The cluster-tree of the Union-Find data structure. The path from a vertex to the root of the cluster-tree is traversed to find the root element in order to differentiate between clusters. The root node of the node-tree is now additionally stored at the root of the cluster-tree. \emph{(b)} The spanning-tree $\vset$ with 9 non-trivial vertices. As $\vset$ is strictly acyclic, the cluster's edges must be maintained such that no cycles are created. This is done during growth by removing edges (red dotted lines) if a cycle is detected. \emph{(c)} The node-tree $\nset$, which currently has the same number of elements as $\vset$, as all vertices are non-trivial. Two depth-first searches are required to compute node parities (head recursively) and delays (tail recursively) in $\nset$.\label{fig3}}

The Partitioned-Growth data structure does not replace but coexists with the Union-Find data structure, whose \textbf{cluster-trees}' critical goal is to differentiate between clusters \cite{delfosse2017almost}. The relevant data structures and the cluster-tree, spanning-tree $\vset$ and node-tree $\nset$ of an example cluster are depicted in \Cref{fig3}. Note that the set of all spanning-trees $\{\vset\}$ is equivalent to the spanning-forest $F$, which is constructed in the original Union-Find decoder after cluster growth and before peeling. When all clusters are of even parity, the set of spanning-trees $\{\vset\}$ can be passed to the peeling decoder \cite{delfosse2017linear}. 


\subsection{Node parity and delay}\label{sec:paritydelay}

The Partitioned-Growth data structure allows for prioritizing the growth of \emph{nodes} on an odd-parity node-tree $\dot{\nset}$ with low PWM, instead of \emph{vertices}. To quantize this priority, we define the \textbf{node delay} $\delta_N$, the number of growth iterations for a node to wait for all nodes in the node-tree to have equal PWM: 
\begin{equation}\label{eq:delayequation}
    \delta_N = 2\left(\text{PMW}(N) - \min_{X\in\nset}{\text{PMW}(X)}\right).
\end{equation}

To grow a cluster, we loop over all nodes in $\nset$ and grow all boundary vertices of $N$ if $\delta_N=0$. However, to find $\delta_N$, the PMW of all nodes in $\dot{\nset}$ must be known, which is no trivial task as the entire tree must be considered for the calculation in every node. Furthermore, the PWM of a node changes as it and its cluster grows. Instead, we will compute for the difference in node delay between child node and its parent. 

Let any node $\mathbf{R}\in\dot{\nset}$ be the \textbf{root node} of the node tree. As $\dot{\nset}$ is an acyclic tree, $\mathbf{R}$ defines the parent-child relations for all the nodes in $\dot{\nset}$, where any node can only have a single parent, but multiple children. Let $M\geq N$ denote that M is the parent of N. Furthermore, let $\nset_M$ be the sub-tree consisting of all ancestors of $M$ and all descendant branches excluding the branch starting from $(M,N)$, and $\nset_N$ be the sub-tree consisting of all descendant branches of $N$. With this notation, any odd-parity node-tree of $\abs{\dot{\nset}}\geq 3$ can be simplified to $\{\nset_M,M,N,\nset_N\}$, where $(\nset_M, M)$ and $(N, \nset_N)$ can represent any number of edges on $\dot{\nset}$. Based on the node types of $N, M$ and the parity (number of syndrome-nodes) in $\nset_N$, we can deduce the parity of $\nset_M$, since $\dot{\nset}$ must be odd. 

If sub-tree $\nset_N$ is of even parity, denoted by $\overbar{\nset_N}$, it must consist of a combination of an even number of even branches and an even number odd branches, where the parity of the branch refers to the parity of the number of syndrome-nodes. For a hypothetical merger to even parity on either $N$ or $M$, the matching of nodes within $\overbar{\nset_N}$ must be within $\overbar{\nset_N}$, since $N,M$ is a single edge. If sub-tree $\nset_N$ is of odd parity, denoted by $\dot{\nset_N}$, it must consist of a combination of an even number of even branches and an odd number of odd branches. For a hypothetical merger to even parity on either $N$ or $M$, there must always be a matching between the single odd branch within $\dot{\nset_N}$ with either $N$, $M$, or in $\nset_M$. The same is true of an even or odd $\nset_M$.

Using this strategy, we can deduce which edges are required for a matching for a merger on $N$ and $M$ and find the difference in node delays $\delta_N - \delta_M$. For example, if both $N,M$ are syndrome-nodes and $\nset_N$ is even, $\nset_M$ must be odd. The potential matching weights on $\dot{M},\dot{N}$ are thus
\begin{align*}
  \text{PMW}(\dot{M}) &= \rho_{\dot{M}} + (\dot{\nset_M},\dot{M}) + (\dot{M}, \dot{N}) + C \\
  \text{PMW}(\dot{N}) &= \rho_{\dot{N}} + (\dot{\nset_M},\dot{M}) + C,
\end{align*}
where $C$ is a constant equal to the weight of the matching with the sub-trees $\dot{\nset_M}$ and $\overbar{\nset_N}$. Filling these values into \Cref{eq:delayequation} we find that the difference in delay is 
\begin{equation*}
  \delta_{\dot{N}} - \delta_{\dot{M}} = 2\left(\rho_{\dot{N}} - \rho_{\dot{M}} - (\dot{M}, \dot{N})\right).
\end{equation*}

Similarly, the difference in node delay can be deduced for any combination of node types of $N,M$ and the parity of $\nset_N$:
\begin{align*}
  \frac{1}{2}(\delta_N-\delta_M) &= \rho_N - \rho_M - (M,N)  &\dot{M}, \dot{N}, \overbar{\nset_N} \\
  &= \rho_N - \rho_M + (M,N)  &\dot{M}, \dot{N}, \dot{\nset_N} \\
  &= \rho_N - \rho_M + (M,N)  &\dot{M}, \bar{N}, \overbar{\nset_N} \\
  &= \rho_N - \rho_M - (M,N)  &\dot{M}, \bar{N}, \dot{\nset_N} \\
  &= \rho_N - \rho_M - (M,N)  &\bar{M}, \dot{N}, \overbar{\nset_N} \\
  &= \rho_N - \rho_M + (M,N)  &\bar{M}, \dot{N}, \dot{\nset_N} \\
  &= \rho_N - \rho_M + (M,N)  &\bar{M}, \bar{N}, \overbar{\nset_N} \\
  &= \rho_N - \rho_M - (M,N)  &\bar{M}, \bar{N}, \dot{\nset_N} \\
\end{align*}
which can be simplified to 
    %n_d = m_d + \Big\lfloor 2C\big(n_r-\rho_M - (-1)^{n_p}\abs{(n,m)}\big )\Big\rfloor
\begin{multline}\label{eq:pmwdif}
    \delta_N - \delta_M = \\
    2\Big(\rho_N-\rho_M - (-1)^{\pi_N}\abs{(N,M)}\Big), \hspace{.3cm} N\leq M,
\end{multline}
where $\pi_N$ is the \textbf{node parity} of $N$, the number of descendant syndrome-nodes modulo 2. Alternatively, The node parity of a node can also be calculated via the node parities of its children nodes. We define the node parity for syndrome-nodes and junction-nodes. 
\begin{align}
    \pi_{\dot{N}} &= \hspace{.6cm}\left(\hspace{.2cm} \sum_{\mathclap{\forall N \leq \dot{N}}} (1+\pi_N) \right ) \bmod 2, \label{eq:snodeparity}\\
    \pi_{\bar{N}} &= 1 - \left(\hspace{.2cm} \sum_{\mathclap{\forall N \leq \bar{N}}} (1+\pi_N) \right) \bmod 2. \label{eq:jnodeparity}
\end{align}

Immediately after, a \textbf{pseudo-delay} $\tilde{\delta_N} = \delta_N - \delta_\mathbf{R}$, where $\mathbf{R}$ is the root of $\nset$, can be calculated by
\begin{equation}\label{eq:pseudodelay}
    \tilde{\delta_N} = \tilde{\delta_M} + (\delta_N - \delta_M), \hspace{.3cm} N\leq M.
\end{equation}
From the pseudo-delay, we can go back to $\delta_N$ by subtracting the minimal value within $\nset$, similarly to \Cref{eq:pmwdif}
\begin{equation}\label{eq:delaypseudo}
    \delta_N = \tilde{\delta_N} - \min_{X \in \nset}{\tilde{\delta_X}}. 
\end{equation}

% \Cref{eq:pmwdif} can be easily explained through an example of the cluster in \Cref{fig1}. The PMW's of $A, B, C$ are respectively 
% \begin{align}
%     \nonumber \text{PMW}(A) &= \rho_A + \abs{(B, C)}, \\
%     \nonumber \text{PMW}(B) &= \rho_B + \abs{(A, B)} + \abs{(B, C)}, \\
%     \nonumber \text{PMW}(C) &= \rho_C + \abs{(A, B)}.
% \end{align}
% Take $A\geq B\geq C$. The differences in node delay are thus 
% \begin{align}
%     \nonumber \Delta_B &= \delta_B - \delta_A =& 2\left(\rho_B - \rho_A + \abs{(A, B)}\right), \\
%     \nonumber \Delta_C &= \delta_C - \delta_B =& 2\left(\rho_C - \rho_B - \abs{(B, C)}\right).
% \end{align}

With \Cref{eq:pmwdif,eq:pseudodelay,eq:delaypseudo,eq:snodeparity,eq:jnodeparity}, it now becomes clear that once the root of $\nset$ is picked, the node parities and differences in node delays can be calculated through two depth-first searches (DFS) of $\nset$. In the first DFS we calculate the node parity $\pi_N$ a via tail-recursive function using \Cref{eq:snodeparity,eq:jnodeparity}, and in the second we calculate the pseudo-delay $\tilde{\delta_N}$ via a head-recursive function using \Cref{eq:pmwdif,eq:pseudodelay}. In the second DFS, the minimal pseudo-delay within $\nset$ can be kept track of by comparison for \Cref{eq:delaypseudo}. Any $\delta_{\mathbf{R}}$, e.g. $\delta_{\mathbf{R}}=0$, can be used as its value is removed in \Cref{eq:delaypseudo}.

\subsection{Growing node-trees}\label{sec:grownodetrees}

A single growth iteration, which is applied in the Union-Find decoder by adding half-edges to all boundary vertices of the cluster, is now replaced by another DFS of $\nset$ from the same root as before. During this DFS, conditionally grow a node --- adding half-edges to the boundary vertices in the current node and adding 1 to its radius $\rho_N$ --- if $\delta_N = 0$. This requires us to save the list of boundary vertices to each node (\Cref{fig3}c). When the delays $\delta_N$ for all nodes in $\nset$ are zero, all nodes are grown simultaneously within the same iteration. 

As the node radii increase, the values for $\delta_N$ across $\nset$ change. This would mean that the DFS's of node parity and delay need to be done after each growth iteration. Luckily, if no new nodes are added to $\nset$ after a growth iteration, which is the case if no mergers occur between clusters, $\pi_N$ on iteration $t+\theta$ can be calculated via previous values of pseudo-delays in iteration $t$ by the introduction of the node \textbf{wait} parameter $\omega_N$. During the growth DFS, if $\delta_N \neq 0$, add 1 to $\omega_N$. If no nodes are added to $\nset$, the node delay in the next iteration are
\begin{multline}\label{eq:delay}
    \delta_N(t+\theta) = \tilde{\delta_N(t)} - \min_{X \in \nset(t)}{\tilde{\delta_X}} - \omega_N(t+\theta-1),  \\
    \hspace{.3cm} N \in \nset, \hspace{.3cm} \nset(t+\theta) = \nset(t).  
\end{multline}

% Note that we have not stated which node in $\nset$ should be the root node. In fact, any node in $\nset$ could have been picked as the root of the node-tree. The only requirement is that the DFS of cluster growth must be performed along the same direction as the DFSs of the parity and delay calculations. If no cluster mergers occur, the node delays can be reused in the node suspension calculation prior to node growth. The node-tree is constructed by storing all neighbors of a node to a list. This way, the DFSs' direction can be determined by simply saving the root node, the starting point of the DFSs, to the cluster. All node variables are depicted in \Cref{fig3}c. 
% In the next section, we expand upon this idea of "reusing" some intermediate parameters to calculate the node suspensions after a cluster merger.  


\subsection{Joining node-trees}\label{sec:nodejoin}

In the Union-Find (UF) algorithm, odd parity clusters of an odd number of non-trivial vertices, which are elements of $\sigma$, grow in size repeatedly and merge with other clusters until all clusters are even. During these mergers, the node-trees of the Partitioned-Growth data structure must also be combined. Let us now first make a clear distinction between the merging protocols of the underlying data structures; the clusters-trees of the UF data structure are merged with the $\Union$ function, whereas the node-trees are merged with a separate $\Nodejoin$ function. After a join of multiple node-trees, the node delays within the combined node-tree change. Therefore, the $\Nodejoin$ protocol's focus is to minimize the DFSs of the recalculation of the node parity and delays in the combined node-tree. 

Let the parity $\nset$ be the number of syndrome-nodes in $\nset$, which is equivalent to the parity of the cluster of $\nset$. For even node-trees, the concept of PMW does not exist, a perfect matching would not be possible when merging with an odd cluster. Consequently, node parity and delays are undefined for even node-trees. 
%Thus, if two odd clusters merge into an even cluster, we don't know and do not care about its node suspensions. 

\Figure[hbt](topskip=0pt, botskip=0pt, midskip=0pt){figures/tikz/build/main-figure4.pdf}{
    \emph{(a)} An odd cluster $\nset_o=\{A, B, \mathbf{O}\}$ with root $A$ joins with an even cluster $\nset_e=\{C, \mathbf{E}\}$ with root $C$ on nodes $\mathbf{O}, \mathbf{E}$, respectively, to a joined node-tree. If we choose to \emph{(b)}, make $\mathbf{E}$ a child of $\mathbf{O}$, the parities and delays the subtree of $\nset_o$ can unchanged, and we only have to perform partial parity and delay calculations over the subtree of $\nset_e$. If we choose to \emph{(c)}, make $\mathbf{O}$ a child of $\mathbf{E}$, parities and delays have to be recalculated in the entire joined node-tree. \label{fig4}}

The second type of merger is between an even and an odd cluster. The combined cluster is odd, and its growth is continued. Thus, its node delays must be computed. Consider the example of odd node-tree $\nset_o$ and even node-tree $\nset_e$ that are to be joined on nodes $\mathbf{O}\in \nset_o$ and $\mathbf{E} \in \nset_e$ (\Cref{fig4}\emph{a}). If $\nset_o$'s root is kept as the root of the joined node-tree (\Cref{fig4}\emph{b}), $\mathbf{E}$ is to be a child node of $\mathbf{O}$. As $\nset_e$ contains an even number of syndrome-nodes, the node parities in $\nset_o$ do not change. Hence, the node parity DFS is only necessary in the subtree $\nset_e$, which now has $\mathbf{E}$ as subroot. Furthermore, as the node delay depends only on its own properties and its parent's, the node delay DFS is also only required from node $\mathbf{E}$ and within the subtree of $\nset_e$. Hence, only \textbf{partial} DFSs within the even cluster are required. % of the node-tree are precisely what was required, as the node parity and delays in $\nset_e$ were undefined. 
Alternatively, if $\nset_e$'s root becomes the root of the combined tree (\Cref{fig4}\emph{c}), an odd number of syndrome-nodes are attached to $\mathbf{E}$, such that the parities of nodes on the path from $\mathbf{E}$ to the root are changed. Such a join would require the DFSs on the entire combined node-tree to calculate for node parities and delays. Thus, a simple rule is always to keep the root of the odd node-tree, which we dub \textbf{Odd-Rooted Join}.

In addition, a cluster can be subjected to multiple mergers within the same growth iteration, during which the parity of the merged cluster changes depending on the number of mergers and the parities of the clusters involved. The DFSs related to the parity and delay calculations must, for this reason, not be initiated directly after the joining of node-trees. After all, it may be possible for the cluster to merge again such that the parities and delays become invalid. To prevent these redundant calculations, subroots of the even subtrees are stored to a list $\m{R}$ at the root of the node-tree (\Cref{fig3}\emph{c}). When multiple mergers occur, the root node that stores the now redundant subroots is replaced by a new root with new $\m{R}$. If a cluster is selected for growth, we bar for the subroots in $\m{R}$ at the new root node and initiate the DFSs from these subroots. We call this the \textbf{Root List Replacement}. 

\Figure[htb](topskip=0pt, botskip=0pt, midskip=0pt){figures/tikz/build/main-figure5.pdf}{
    The node delays $\delta_N$ for nodes for 3 odd node-trees $\{\nset_1, \nset_2, \nset_3\}$ of 3 nodes that grow and join into a single node-tree. \emph{(a)} Node delays are calculated via \Cref{eq:pmwdif,eq:pseudodelay,eq:delaypseudo}. In step 1, the growth in each of the three node-trees' outer nodes is prioritized, and the node-trees merge. In step 2, the recalculation of the joined node-tree is performed. Parities within the subtree of $\nset_2$ are now inverted, and the  in these nodes have doubled. \emph{(b)} Node delays are calculated with \Cref{eq:pmwdifnew} in stead of \Cref{eq:pmwdif}. Now the increase in $\delta_N$ after parity inversion is halved.\label{fig5}}

\subsection{Parity Inversion}\label{sec:inversion}
An unfortunate effect, which we dub \textbf{parity inversion}, causes a decrease in the algorithm's performance as the lattice size is increased. We will demonstrate this effect through the example in \Cref{fig5}\emph{a}. Consider three instances of the node-tree of \Cref{fig0}; $\nset_a, \nset_b, \nset_c$, positioned near each other on the lattice. For each node-tree, if the middle node is suspended from growth for two iterations, all nodes have the same Potential Matching Weight. However, in the current example, the node-trees $\nset_a, \nset_b, \nset_c$ merge after 1 iteration. The combined node-tree is odd. Thus, we recalculate the node parities and delays to find that the parities in the partition of the node-tree containing the nodes of $\nset_b$ have been inverted, and the node suspensions in this partition have doubled with respect to their value before the merger. If the next merging event occurs on the node with the doubled node suspension, the matching weight may be large, in contrast with the goal of decreasing the matching weight. Moreover, the effect of parity inversion accumulates if subsequent mergers occur before zero node suspension has been reached. %Nevertheless, as more inversions occur, the maximum node suspension in the node-tree increases, and it becomes more and more unlikely for a cluster to actually reach zero node suspension in all nodes. The number of inversions is directly related to the number of merging events, and thus to the size of the lattice. The performance to improve the heuristic for minimum weight matching thus decreases for larger lattices. 

%Parity inversion defines a trade-off in the Node-Suspension data structure: `A node must wait as many iterations as it is suspended to reach equilibrium in Potential Matching Weight in the node-tree. However, after Parity Inversion, the node suspension for previously prioritized nodes increases linearly with the number of iterations waited by the suspended nodes pre-inversion.' As a compromise, 
To compensate the effect of parity inversion, we redefine the node delay as \textbf{half} the number of growth iterations needed for all nodes in the node-tree to reach equal PMW. This changes \Cref{eq:delayequation,eq:pmwdif} to 
\begin{align}
    \delta_N &= \text{PMW}(N) - \min_{X\in\nset}{\text{PMW}(X)}, \hspace{.3cm} N \in \nset, \tag{\ref{eq:delayequation}'}  \\
    \delta_N - \delta_M &= \rho_N-\rho_M - (-1)^{\pi_N}\abs{(N,M)}, \hspace{.3cm} N\leq M. \tag{\ref{eq:pmwdif}'}\label{eq:pmwdifnew}
\end{align}

\subsection{Algorithm description}\label{sec:pseudocode}
\begin{algorithm}[htb]
  \BlankLine
  \KwData{A graph $G=(V,E)$, and syndrome $\sigma \subseteq V$}
  \KwResult{Correction set $\m{C}$}
  \BlankLine
  Initialize cluster spanning-trees and node-trees.\;\label{algo:B1a}% and table $\Support$
  Create the list $\m{L}$ of odd clusters.\;
  \While(){$\m{L}$ is not empty}{
    Initialize the fusion list $\m{F}$ as an empty list.\;\label{algo:B1b}
    \For(){cluster $\in\m{L}$ \label{algo:B2a}}{
      \For(){$N \in\m{R}$ at root node $\mathbf{R}$ of $\nset$}{
        Apply DFSs to calculate node parities and delays (\Cref{eq:pseudodelay,eq:snodeparity,eq:jnodeparity}) from $N$ to all descendant nodes. Keep track of $\min_{X \in \nset}{\tilde{\delta_X}}$ during the delay DFS.\;\label{algo:pdc}
      }
      Apply DFS from root $\mathbf{R}$ to all descendants. At each node $N$ during the DFS, if $\delta_N=0$ (\Cref{eq:delay}), grow all boundary edges of vertices in the node a half-edge per the Union-Find decoder, such that grown edges are added to $\m{F}$, and apply $\rho_N=\rho_N +\nicefrac{1}{2}$. \;\label{algo:grow}
      %If $\m{s}(n)\neq0$, apply $n_w=n_w+1$ and continue the DFS.
    }
    \For(){edge $(n,m) \in \m{F}$\label{algo:B3a}}{
      \eIf(){$\Find(n)\neq\Find(m)$}{
        Merge cluster-trees by Weighted $\Union$.\;
        \eIf(){$n \in N$ and $m \in M$\label{algo:joina}}{
          Merge node-trees by Odd-Rooted $\Nodejoin$. If resulting node-tree with root $\mathbf{R}$ is odd, add even subroot (either $N$ or $M$) to list $\m{R}$ at $\mathbf{R}$.\;
        }($n$ or $m$ does not belong to a cluster){
          Add $n$ to $N$ or $m$ to $M$.\;\label{algo:joinb}
        }
      }($n,m$ in same cluster.\label{algo:dfa}){
        %Subtract 1 from $(u,v)$ in $\Support$.\;
        Nothing to keep both $\vset$ and $\nset$ acyclic.\;\label{algo:dfb} 
      }
    }
    Update $\m{L}$ with odd clusters\; \label{algo:B3b}
  }
  Apply the peeling decoder \cite{delfosse2017linear}.\label{algo:B4a}
  \caption{Union-Find Partitioned-Growth \footnotesize decoder}\label{algo:ufbb}
\end{algorithm}

The full version of the algorithm we have described is given in Algorithm \ref{algo:ufbb}. Note that this pseudocode includes instructions that are shortened versions of the pseudocode of the Union-Find decoder \cite{delfosse2017almost}. This is done for clarity on the modified additions of the Partitioned-Growth data structure and protocols on top of the Union-Find pseudocode. The first block of lines \ref{algo:B1a}-\ref{algo:B1b} initializes the clusters and describes the loop of cluster growth. Block 2 contains lines \ref{algo:B2a}-\ref{algo:grow} and describes the DFSs related to calculating the node parities and delays from all even subroots stored at the root node, and the DFS of the cluster growth. Block 3 contains lines \ref{algo:B3a}-\ref{algo:B3b} and describes the combined merging protocols of the Union-Find and Partitioned-Growth data structures. Note that lines \ref{algo:dfa}-\ref{algo:dfb} contain an extra step that maintains acyclic spanning-trees. The final block in line \ref{algo:B4a} is the peeling decoder \cite{delfosse2017linear}, which now does not have to create the spanning forest of the grown clusters. Similarly to the Union-Find decoder, Weighted Growth is applied such that the smaller cluster is always grown first. 
